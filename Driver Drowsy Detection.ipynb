{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6ebcf3-98bf-40a5-b2ce-8fde0e89008d",
   "metadata": {},
   "source": [
    " # Driver Drowsy Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89e7b8-0379-4977-ad08-1c53ff8f706d",
   "metadata": {},
   "source": [
    "\n",
    "The provided code appears to be a Python script that likely involves facial recognition and possibly some audio output using the Pygame library. Let's break down the code and provide an explanation for each import statement:\n",
    "\n",
    "import imutils:\n",
    "imutils is a package that simplifies various image processing tasks in OpenCV, such as resizing, rotating, and displaying images. It provides a set of convenience functions to work with OpenCV, making it easier to handle images.\n",
    "\n",
    "import dlib:\n",
    "dlib is a popular library for machine learning and computer vision tasks. It includes tools for facial recognition and shape prediction among other things.\n",
    "\n",
    "import cv2:\n",
    "cv2 is the OpenCV library, which is widely used for computer vision and image processing tasks. OpenCV provides a variety of functions for handling images and video streams, including face detection and other computer vision tasks.\n",
    "\n",
    "from scipy.spatial import distance:\n",
    "This line imports the distance function from the scipy.spatial module. It can be used to compute various types of distances between points or arrays in space, which may be relevant in this script for facial feature analysis or distance calculations.\n",
    "\n",
    "from imutils import face_utils:\n",
    "This line imports the face_utils module from the imutils package. face_utils likely contains utility functions to work with facial features detected in an image, possibly in conjunction with dlib.\n",
    "\n",
    "from pygame import mixer:\n",
    "This line imports the mixer module from the Pygame library. Pygame is a popular library for creating 2D games and multimedia applications in Python. In this case, it suggests that the script may involve audio output or sound effects using Pygame's mixer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d210d598-3698-42e1-a9ed-203aacc11649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from pygame import mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3db3a-e4be-4db2-8c3d-e8cfb1fe0d52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "mixer.init(): This is likely using the mixer module from the popular Python library, pygame. \n",
    "It initializes the mixer system, which is typically used for handling sound and music.\n",
    "mixer.music.load(\"music.wav\"): This line loads an audio file named \"music.wav\" for playback.\n",
    "It seems to set up the music to be played later in the script.\n",
    "\n",
    "scipy.spatial to calculate Euclidean distances between points.\n",
    "\n",
    "eye: This parameter is expected to be a list or array of eye landmarks, typically represented as (x, y) coordinates. The assumption is that eye contains points for both the left and right eyes.\n",
    "\n",
    "A, B, and C: These variables represent distances between specific pairs of landmarks, which are used to calculate the EAR.\n",
    "\n",
    "EAR (eye_aspect_ratio): EAR is a measure of how open or closed an eye is. It is calculated as the average of two ratios: A+C and B. This calculation is based on research in the field of computer vision and is often used in tasks like blink detection and drowsiness detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e8e229-dffb-4deb-ab1b-59c5d62a47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mixer.init()\n",
    "mixer.music.load(\"music.wav\")\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "\tA = distance.euclidean(eye[1], eye[5])\n",
    "\tB = distance.euclidean(eye[2], eye[4])\n",
    "\tC = distance.euclidean(eye[0], eye[3])\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\treturn ear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78839ec2-4235-4c32-ade6-4df7966ec4e5",
   "metadata": {},
   "source": [
    "thresh = 0.25: This line sets a threshold value to 0.25. The threshold is a parameter that can be used for various purposes, and its specific purpose is not evident from this snippet. It might be used for some kind of comparison or decision-making within the code.\n",
    "\n",
    "frame_check = 20: This line sets the variable frame_check to the value 20. It's likely used to control or count the number of frames for some operation or condition within the code.\n",
    "\n",
    "detect = dlib.get_frontal_face_detector(): Here, you are using the dlib library to create a face detector object. The get_frontal_face_detector() function initializes a face detector using the dlib library, which can be used to locate and detect faces in images or video frames.\n",
    "\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\"): This line creates a facial landmark predictor using a pre-trained model. The \"shape_predictor_68_face_landmarks.dat\" file likely contains a pre-trained model for predicting facial landmarks on a face. This predictor is used to identify specific facial features like eyes, nose, mouth, etc.\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]: Here, you seem to be extracting the indices of the facial landmarks corresponding to the left eye from a constant or dictionary named FACIAL_LANDMARKS_68_IDXS. This is part of a common practice when working with facial landmarks to define specific regions of interest on the face.\n",
    "\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]: Similar to the previous line, this extracts the indices of the facial landmarks corresponding to the right eye.\n",
    "\n",
    "cap = cv2.VideoCapture(0): This line initializes a video capture object using OpenCV. It sets up a connection to the default camera (camera index 0). This is typically used to capture frames from a webcam or other video sources.\n",
    "\n",
    "flag = 0: Initializes a variable flag and sets its value to 0. The purpose of this flag is not clear from the provided code snippet, but it's likely used for some control or decision-making within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8af5bf7-3b26-4280-ab03-695520eedf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.25\n",
    "frame_check = 20\n",
    "detect = dlib.get_frontal_face_detector()\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]\n",
    "cap=cv2.VideoCapture(0)\n",
    "flag=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9352eba-0abf-4f19-b782-a326d66f0711",
   "metadata": {},
   "source": [
    "This section begins an infinite loop where frames from a video capture source (cap) are continuously read.\n",
    "The imutils.resize function is used to resize the frame to a width of 450 pixels.\n",
    "The frame is converted to grayscale to simplify processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1e71b-2be2-44c0-957f-96aa800cdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tret, frame=cap.read()\n",
    "\tframe = imutils.resize(frame, width=450)\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tsubjects = detect(gray, 0)\n",
    "\tfor subject in subjects:\n",
    "\t\tshape = predict(gray, subject)\n",
    "\t\tshape = face_utils.shape_to_np(shape)\n",
    "\t\tleftEye = shape[lStart:lEnd]\n",
    "\t\trightEye = shape[rStart:rEnd]\n",
    "\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
    "\t\trightEAR = eye_aspect_ratio(rightEye)\n",
    "\t\tear = (leftEAR + rightEAR) / 2.0\n",
    "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
    "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
    "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\t\tif ear < thresh:\n",
    "\t\t\tflag += 1\n",
    "\t\t\tprint (flag)\n",
    "\t\t\tif flag >= frame_check:\n",
    "\t\t\t\tcv2.putText(frame, \"###########ALERT!###########\", (10, 30),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\tcv2.putText(frame, \"###########ALERT!###########\", (10,325),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\t\t\t\tmixer.music.play()\n",
    "\t\telse:\n",
    "\t\t\tflag = 0\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00fc0d-d9ff-4aff-b29c-d8c3bce851b0",
   "metadata": {},
   "source": [
    "A function called detect is called with the grayscale frame as input, which likely performs face detection. The 0 is a parameter that might control the sensitivity or threshold for face detection.\n",
    "A function called detect is called with the grayscale frame as input, which likely performs face detection. The 0 is a parameter that might control the sensitivity or threshold for face detection.\n",
    "For each detected face (subject), the code calls a function predict to estimate facial landmarks (facial feature points). These points are then converted to NumPy arrays using face_utils.shape_to_np.\n",
    "The code extracts the left and right eye landmarks from the estimated facial landmarks and calculates the Eye Aspect Ratio (EAR) for each eye. EAR is a measure of how open the eyes are. The EAR values for both eyes are averaged to get a single EAR value for the face.\n",
    "Convex hulls are computed for the left and right eyes, and the code draws green contours around the eyes on the frame.\n",
    "The code checks if the calculated EAR is less than a certain threshold (thresh). If the condition is met, it increases a flag counter, and if the flag counter exceeds a predefined value (frame_check), it adds an \"ALERT!\" message to the frame, plays a sound using the mixer.music.play() method, and increments the flag.\n",
    "The processed frame with eye contours and alert messages is displayed, and the code waits for a keypress. If the 'q' key is pressed, the loop is exited.\n",
    "After breaking out of the loop, the OpenCV windows are destroyed, and the video capture source is released."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4198d-2e4f-4955-b0b8-b703f86be4f3",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9bb65-bd97-42d9-8dd9-74afc4e5f2fa",
   "metadata": {},
   "source": [
    "\n",
    "This code is typically used for monitoring a person's drowsiness in real-time from a video feed, and it triggers alerts and sounds when drowsiness is detected based on eye movement patterns. You would need to provide the missing parts of the code (functions like detect, predict, and variables like lStart, lEnd, rStart, rEnd, thresh, and frame_check) for a complete understanding and execution of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9db0d9-d774-4843-83ca-61ec6a951549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
